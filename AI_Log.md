# AI Questions Log

**Question:**
```
Tell me about API Gateway in a loosely coupled microservice saga architecture
```

**Response:**
Great question â€” this touches on several important architectural concepts: API Gateway, Microservices, Loose Coupling, and the Saga Pattern for distributed transactions. Let's unpack all of these together, and then show how the API Gateway fits in the big picture.

ğŸ§© 1. Overview: Where API Gateway Fits

In a microservice architecture, you typically have multiple independent services â€” each handling a specific domain (e.g. user service, order service, payment service).

Clients (mobile apps, web frontends, external APIs) need to communicate with these services.

But directly exposing every service to the outside world leads to:
- Too many endpoints for clients to manage
- Security and authentication complexity
- Tight coupling between frontend and backend service structure
- Harder versioning and monitoring

âœ… The API Gateway acts as a single entry point for all client requests, routing them to the correct microservice while providing cross-cutting concerns like authentication, logging, rate limiting, and aggregation.

ğŸ§± 2. What an API Gateway Does

| Feature | Description |
|---------|-------------|
| Routing | Directs incoming requests (like /orders/123) to the appropriate microservice (Order Service). |
| Aggregation | Combines data from multiple services into one response (e.g. Order + Payment + Shipping status). |
| Authentication & Authorization | Validates tokens (JWT, OAuth2, etc.) before requests reach microservices. |
| Load Balancing | Distributes requests across multiple service instances. |
| Caching | Reduces load and latency for frequently requested data. |
| Rate Limiting / Throttling | Protects services from being overloaded. |
| Monitoring / Logging | Provides centralized request/response logging and tracing (often integrated with OpenTelemetry or Zipkin). |
| API Versioning | Handles multiple versions of APIs cleanly. |

ğŸ§© 3. API Gateway in a Loosely Coupled System

Loose coupling means:
- Services do not depend directly on each other's internal logic or databases.
- They communicate asynchronously (via events or message queues) when possible.
- Each service can evolve, scale, or fail independently.

The API Gateway reinforces this by:
- Decoupling clients from direct service contracts.
- Allowing backend services to change their internal APIs without breaking the public API.
- Using facade pattern: the Gateway presents a stable external interface while hiding backend changes.

Example:
Frontend calls POST /api/v1/order.
The Gateway knows it needs to call:
- Order Service to create the order
- Payment Service asynchronously (via Kafka or RabbitMQ) to process payment

Frontend never needs to know these details.

ğŸ”„ 4. API Gateway in a Saga Architecture

Now let's connect this to Saga, which handles distributed transactions across multiple services.

âš™ï¸ Saga Pattern Recap

When you need a multi-step operation that touches several services (like Order + Payment + Inventory), and there's no global ACID transaction, you use Saga â€” a sequence of local transactions coordinated either:
- Choreography (Event-based): Each service listens to events and produces its own next event (no central coordinator).
- Orchestration: A Saga orchestrator service tells each service what to do next.

Example saga (Order flow):
1. API Gateway receives POST /order
2. Gateway routes to Order Service
3. Order Service starts saga:
   - Save pending order
   - Publish OrderCreated event
4. Payment Service consumes event â†’ reserves funds â†’ emits PaymentConfirmed
5. Inventory Service consumes event â†’ reserves items â†’ emits InventoryReserved
6. Order Service listens for both confirmations â†’ marks order as complete
7. On failure, compensating actions are triggered (refund, release stock)

ğŸ¯ Role of API Gateway in Saga

The API Gateway does not orchestrate sagas â€” that's the job of internal coordination or messaging.
But it initiates them and reports their status to clients.

So in a saga setup:
- Gateway â†’ receives client request â†’ forwards to Order Service (saga initiator)
- Gateway may offer endpoints for querying saga progress (GET /order/{id}/status)
- Gateway handles retries and timeouts for external clients
- Gateway can expose a consistent API while saga executes asynchronously

ğŸ§  5. Example Flow Diagram

```
[Client App]
     |
     v
 [API Gateway]
     |
     +--> [Auth Service] -- JWT validation
     |
     +--> [Order Service] -- starts saga
             |
             +--> [Payment Service]
             +--> [Inventory Service]
             +--> [Shipping Service]
```

Communication between internal services happens asynchronously (via Kafka/RabbitMQ/Event Bus).
The API Gateway only knows about the entry point and result endpoints, keeping coupling minimal.

âš™ï¸ 6. Implementation Options

Popular API Gateway Tools

| Platform | Examples |
|----------|----------|
| Cloud-native | AWS API Gateway, Azure API Management, Google Cloud Endpoints |
| Self-hosted / Open Source | Kong, Traefik, NGINX, Envoy, KrakenD, Ocelot (.NET) |
| Service Mesh Integration | Istio, Linkerd (works with sidecar proxies for deeper traffic control) |

For microservices using event-driven Saga, Kong + Kafka or Traefik + NATS/RabbitMQ is a common pattern.

âœ… 7. Key Takeaways

| Concept | Summary |
|---------|---------|
| API Gateway | Single, secure entry point for all external clients. |
| Loose Coupling | Gateway hides internal service contracts and allows backend evolution. |
| Saga Pattern | Manages distributed transactions across microservices. |
| Gateway's Role in Saga | Initiates requests and exposes APIs for saga status; does not coordinate saga logic. |
| Result | A scalable, maintainable, and fault-tolerant architecture. |
---

**Question:**
```
What happens if a message fails in RabbitMQ and goes to a Dead Letter Queue?
```

**Response:

This is a fun topic because it blends together some of the most important parts of event-driven architecture: message reliability, retries, failure handling, and DLQs (Dead Letter Queues).
Letâ€™s break it down step by step and understand what really happens behind the curtain when a message â€œdiesâ€.

## 1. Overview: Why Messages Fail in Event-Driven Systems

In a loosely-coupled microservice architecture, services communicate through events.
But sometimes messages canâ€™t be processed successfully:

consumer is down

data is invalid

schema changed

business rule fails

message expires

it gets retried too many times

We canâ€™t let a single bad message block the entire queue or crash the consumer.

This is exactly why RabbitMQ uses DLQs.

## 2. What Exactly Is a Dead Letter Queue (DLQ)?

A DLQ is a special holding queue for messages that:

were rejected (with requeue = false),

expired (TTL exceeded),

hit max retry attempts,

or exceeded the max queue length.

Instead of losing those messages â€” RabbitMQ saves them.

Think of DLQ as:

### "The waiting room for failed messages"
### where engineers can inspect them
### and optionally replay them later.

## 3. What Happens Internally When a Message Fails?

Hereâ€™s the real flow, simplified:

Consumer tries to process a message

Something goes wrong (exception, timeout, invalid dataâ€¦)

Consumer tells RabbitMQ:
"reject this message and donâ€™t requeue it"

RabbitMQ immediately routes the message to the Dead Letter Exchange (DLX)

The DLX sends it to the Dead Letter Queue (DLQ)

Message stays there until someone:

reads it

fixes it

republishes it

or discards it

This prevents the main queue from being clogged by poison messages.

## 4. Why DLQs Are So Important in Microservice Saga Architecture

In a Saga pattern, services depend heavily on event consistency.

If a message like:

OrderCreated

PaymentCompleted

BookingCancelled

fails to be processed, the entire Saga could break.

DLQs make the system fault-tolerant:

They avoid infinite retry loops

They isolate â€œbad eventsâ€

They give engineers a chance to repair the flow manually

They keep the saga state consistent

They prevent cascading failures across microservices

Without DLQ, one corrupted event could break the entire chain.

## 5. What You Usually Do With Messages Inside a DLQ

Engineers or automated workers typically:

1. Inspect

Check why the message failed â€” bad schema? missing field?

2. Log

Store it for audit or debugging.

3. Fix

Correct payload or update consumer logic.

4. Replay

Send it back to the original exchange after fixing the issue.

DLQ = safe storage â†’ nothing is lost until you decide what to do with it.

## 6. Example Failure Scenario

Imagine the Payment Service publishes:

{
  "paymentId": "123",
  "amount": "INVALID_VALUE"
}


Now:

PaymentCompleted â†’ BookingService

BookingService tries to parse amount â†’ throws exception â†’ rejects message â†’ message goes to DLQ.

Saga continues functioning for all other orders, only this broken event is isolated.

Later, a developer inspects DLQ, fixes the payload, and republishes it.

## 7. Key Takeaways
Concept	Summary
DLQ	A safe storage for messages that failed to process
Prevents system crashes	Stops poison messages from blocking queues
Used in saga architecture	Protects distributed workflows from broken events
Messages are not lost	They can be inspected, fixed, replayed
Essential for reliability	Ensures event-driven systems behave predictably under failure


---


**Question:**
```
How does this architecture handle failures and ensure reliability across multiple microservices?
```

**Response:**
ğŸŸ¦ 1. Preventing Cascading Failures with Circuit Breakers

The system uses Resilience4j Circuit Breakers to prevent failures in one service from propagating to others.

Detects when a downstream service becomes slow or unavailable

Opens the circuit to stop sending new requests

Allows the failing service time to recover

Explicitly used in the Exam Service â†’ Notification Service integration

This ensures that a single failing component does not disrupt the whole system.

ğŸŸ© 2. Handling Transient Errors with Retry and Exponential Backoff

For temporary network or database problems, the architecture applies:

Automatic retry attempts

Increasing delay between retries (exponential backoff)

Protection against flooding the failing service

Ideal for services with external dependencies

This significantly improves stability during short-lived outages.

ğŸŸ§ 3. Message Durability and Failure Isolation via RabbitMQ

All asynchronous communication is handled through RabbitMQ, which adds built-in resilience:

Durable messages

Acknowledgement (Ack/Nack) for safe processing

Automatic retry mechanisms

Dead Letter Queues (DLQs) for messages that repeatedly fail

Guarantees no messages are lost even under high load or service downtime

RabbitMQ forms the reliability backbone of the system.

ğŸŸ¨ 4. Distributed Transaction Reliability via Saga (Choreography)

For workflows involving multiple services, the system uses the Saga Pattern (Choreography):

Each service publishes domain events

Other services react independently

No central orchestrator needed

Compensation actions for failure scenarios

Exact example in the project:

Marketplace â†’ Payment â†’ Marketplace

This prevents locking, reduces coupling, and guarantees eventual consistency.

ğŸŸ« 5. Failure Isolation with Database per Service

Each microservice has its own dedicated PostgreSQL instance, ensuring:

Strong data isolation

No direct cross-service database access

A database failure only affects its own service, not others

This design dramatically increases system reliability.

ğŸŸ¥ 6. Reliability Support via Redis Cache

Redis improves resilience in several ways:

Reduces load on core services (Auth, Booking)

Enables rate limiting to prevent overload

Stores JWT blacklists for secure authentication

Provides ultra-fast responses during system strain

Redis effectively acts as a stabilizer under high traffic.

ğŸŸª 7. Loose Coupling Through Event-Driven Architecture

Because communication is asynchronous and event-based:

Services do not depend on each otherâ€™s availability

Messages are stored until consumers are ready

There is no synchronous dependency chain

Services remain operational even if others fail temporarily

---
**Question:**
```
How is the system architecture designed to ensure scalability and reliability in the project?
```
**Response:**
The system architecture is designed using microservices to ensure scalability and reliability. Specifically:

Scalability:
The architecture supports horizontal scaling by adding more resources as the number of concurrent users increases (e.g., during class reservations). This is achieved through the use of load balancers and stateless services, which allow for easy distribution of workloads.
The system can support an unlimited number of "sellers" and "products" in the marketplace module. This is facilitated by the distributed nature of the system and its ability to scale resources as needed.

---

**Question:**


```How does the architecture maintain security and ensure data consistency between critical services such as the Auth Service, User Service, and API Gateway?```

**Response:**
Security is enforced via JWT tokens issued by the Auth Service.  
The API Gateway validates all incoming tokens, applies rate-limiting, and forwards only authenticated requests to internal services. This centralizes authentication and keeps services focused solely on authorization.  
Data consistency is maintained asynchronously: when a user registers, the Auth Service publishes a UserRegistered event through RabbitMQ.  
The User Service listens to this event and creates the user profile inside its own PostgreSQL database.  
Any updates to roles or profiles are also propagated through events, allowing all services to stay eventually consistent while remaining fully decoupled.

---


**Question:**


```Why does each microservice in the project have its own dedicated PostgreSQL database instead of sharing a single central database?```


**Response:**


Each microservice represents a separate business domain with its own data ownership and internal rules.  
Using a dedicated PostgreSQL database per service enforces strong domain boundaries and prevents unwanted coupling between services.  
This separation allows teams to evolve their schemas independently without risking cross-service conflicts.  
It also increases reliability: if one database becomes slow or faces issues, other services remain fully operational.  
Finally, isolated data storage is essential for event-driven architectures, enabling services to maintain local transactional integrity while synchronizing changes through domain events.
---

**Question:**
```
In the given microservices architecture, which uses an event-driven Saga (Choreography) pattern with RabbitMQ, how does the system ensure data consistency across services such as Marketplace, Payment, and Booking without relying on distributed transactions, and what failure-handling mechanisms prevent the Saga from breaking under partial failures?
```
**Response:**
The system maintains data consistency across Marketplace, Payment, and Booking by using an event-driven Saga (Choreography) instead of distributed ACID transactions. Each service independently performs its local transaction and then emits an event indicating the new state. Other services react to these events and execute their own compensating or follow-up actions.

Here is how consistency is ensured:

1. Local Transactions Only

Each microservice owns its own PostgreSQL database, so every step of the Saga is a fully isolated local transaction.
There is no shared database and no distributed locks.

2. Domain Events via RabbitMQ

After completing its local transaction, a service publishes an event such as:

OrderCreated

PaymentCompleted

PaymentFailed

BookingConfirmed

Other services subscribe and react to those events. This creates an event chain that forms the Saga.

3. Compensating Actions

If a step fails, services emit compensating events instead of rolling back globally.
Example:

If Payment fails â†’ Payment Service publishes PaymentFailed

Marketplace Service reacts and marks the order as Cancelled

Notification Service sends an email accordingly

4. Fault Tolerance Mechanisms

The architecture includes several safety layers:

a. Dead Letter Queues (DLQ)

Failed messages go to a DLQ in RabbitMQ where they can be inspected and manually replayed.
This prevents message loss and Saga desynchronization.

b. Retry with Exponential Backoff

Transient failures (like network glitches) are retried automatically.
This avoids unnecessary compensations.

c. Circuit Breaker (Resilience4j)

Used especially in services like Exam â†’ Notification.
If an external dependency fails repeatedly, the breaker opens and stops propagating the failure.

d. Idempotent Consumers

Event handlers apply updates safely even when the same event is delivered more than once.

e. Optimistic Locking

Booking Service uses @Version to prevent race conditions and overbooking.

5. Eventual Consistency

Because each service updates itself asynchronously, the system doesnâ€™t guarantee immediate consistencyâ€”but it ensures eventual consistency through:

deterministic event flows

compensations

replayable messages

---
**Question:**
```
How can we structure the Saga for booking a resource so that overbooking is prevented even when multiple requests arrive concurrently across multiple instances?
```

**Response:**
Use a reservation-based Saga (reserve â†’ confirm â†’ commit) where:

Booking Service initiates the Saga and reserves capacity on the Resource Service (a local atomic decrement / row lock).

Reserve step creates a short-lived reservation record (with TTL) in Resource DB (or Redis) and is transactional in the Resource Service.

If reserve succeeds, Booking Service proceeds to payment/confirm steps; if anything fails or times out, the Saga issues a compensating cancel to release the reservation.

Use idempotent operations, unique constraints, and local ACID operations (row locks or atomic counter decrements) to make reservation non-racy.

Persist events using the Transactional Outbox pattern in each service and publish via RabbitMQ to avoid lost messages.

This preserves eventual consistency via Saga events and prevents overbooking by enforcing the constraint at the resource owner (Resource Service) using local atomicity.

Monitoring, observability, and testing

Trace IDs: Attach a correlation ID to the Saga. Propagate it in all events and logs so you can trace a whole Saga across services.

Metrics: publish metrics for reservations_created, reservations_confirmed, reservations_cancelled, reservation_timeouts, and overbooking_attempts.

Alerts: alert on reservation_timeouts spikes and capacity_available negative attempts.

Integration tests: simulate concurrent booking requests (100s parallel) for same resource and assert capacity_available >= 0 and no more CONFIRMED reservations than capacity_total.

Chaos tests: introduce delays/failures in Resource Service and verify compensations and TTL logic work.

Alternatives & tradeoffs

Centralized lock service (Redis Redlock): serializes access but adds dependency/complexity and single point of failure risk; still requires DB enforcement.

Two-phase commit (2PC): not allowed given microservice autonomy and requirement to avoid distributed transactions.

Choreography-only Saga: less centralized but harder to ensure strict capacity constraints; not recommended where strict no-overbooking is required.

Optimistic with retries: reduces locking contention but requires backoff and retries; acceptable if conflicts are rare.

Failure scenarios & recovery

Resource Service down after Reserve but before Confirm: reservation TTL will expire and restore capacity; Booking Service should have retry/backoff and cancel logic.

Booking Service crashes after Reserve: when Booking Service restarts it should reconcile pending bookings (query reservations) and either confirm or cancel.

Message loss: transactional outbox + durable RabbitMQ queues prevents loss; consumer idempotency protects against duplicates.
Checklist for implementation

Resource Service enforces capacity changes atomically in local transactions.

Reservation rows created with TTL and unique booking_id.

Booking Service acts as Saga orchestrator with clear state machine (PENDING â†’ CONFIRMED/CANCELLED).

Idempotency keys and correlation IDs present on all commands/events.

Transactional outbox pattern implemented in both services.

RabbitMQ exchanges/queues configured durable + appropriate routing (direct/topic).

Background job to cancel expired reservations and publish events.

Tests: concurrency load tests, integration tests, chaos tests.

Observability: tracing, reservation metrics, alerts.
---

**Question:**
What is the role of the API Gateway in JWT validation ?

**Response:**
When youâ€™re running a microservice architecture â€” especially one thatâ€™s event-driven, loosely coupled, and riddled with distributed transactions â€” you never want each service to individually struggle with authentication and token verification.

Thatâ€™s where the API Gateway steps in.
Think of it as the immune system of your architecture: nothing enters unless it gets scanned, verified, and approved.


ğŸ§© 1. Why JWT validation shouldnâ€™t be inside each service

In a distributed system with:

10+ microservices

independent deployments

independent scaling

different domains

â€¦forcing every service to validate JWT means duplication, inconsistency, and security drift.

You donâ€™t want a scenario where:

User Service uses expired key set

Booking Service uses an outdated algorithm

Marketplace validates only the signature but not the scopes

Chaos emerges. Attackers love this kind of inconsistency.

ğŸšª 2. The API Gateway becomes the â€œsecurity bouncerâ€

Every request must pass through the Gateway before reaching any service.

The Gateway checks:

Is the token present?
Missing? Reject immediately.

Is the signature valid?
Compare against Auth Serviceâ€™s public key.

Is the token expired?

Has it been blacklisted? (Redis)

Does the user have the required roles/permissions?

Microservices remain clean, lightweight, and protected.

ğŸ” 3. What the API Gateway actually validates

JWT validation usually includes:

Signature verification (public key check)

Issuer check (iss)

Audience check (aud)

Expiration check (exp)

Not-before check (nbf)

Scopes / roles / permissions

Token blacklist lookup (logout, compromised tokens)

In your architecture, Redis is the real hero behind the scenes for blacklist storage.

ğŸ§± 4. Why the Gateway is perfect for this job

Because it owns all the cross-cutting concerns:

Authentication

Authorization

Rate limiting

Routing

Logging

Monitoring

Request transformation

JWT validation fits naturally into this layer.

Microservices donâ€™t waste CPU cycles parsing JWTs; they simply trust:

â€œIf the API Gateway forwarded this request, the user is verified.â€

ğŸ”„ 5. Flow of JWT validation in your architecture

Letâ€™s walk the request path:

Client --> API Gateway --> Microservice


Step-by-step:

Client sends request with Authorization: Bearer <JWT>

Gateway extracts token

Gateway validates token signature using Auth Serviceâ€™s public key

Gateway checks blacklist in Redis

Gateway checks RBAC rules for the endpoint

Only if valid â†’ Forward to microservice

Microservices now trust the Gateway and operate in a fully secured zone.

ğŸ¯ 6. Why this matters in a Saga architecture

In Saga (choreography):

Services talk to each other via RabbitMQ

Not every event needs a JWT

Internal trust boundaries are different from external ones

If JWT validation was inside each service, you'd need to validate tokens for events too â€” which makes no sense.

Gateway solves this elegantly:

ğŸŸ¢ External requests â†’ validated
ğŸŸ¡ Internal events â†’ trusted network, donâ€™t require JWT
ğŸ”´ Unauthorized user â†’ blocked instantly at the edge

This keeps the Saga flow clean and secure.

ğŸ§  7. Key Takeaways
Concept  Summary
API Gateway  First layer of defense, validates JWT before routing
JWT Validation  Checks signature, expiry, roles, blacklist, etc.
Microservices  Stay stateless and trust Gateway
Redis  Stores blacklisted tokens
Saga Pattern  Internal events bypass JWT; external calls are secured at the edge
---


**Question:**
```
How can I design a product management system for the Marketplace service?
```

**Response:**
To design a product management system for the Marketplace service:

Create a Product Service: This service will handle the creation, updating, deletion, and retrieval of products. 

Use a Database: Store product information like name, price, category, and description in a database (such as PostgreSQL). 

Implement CRUD Operations: Create endpoints for Create (POST), Read (GET), Update (PUT), and Delete (DELETE) operations for products. 

Use Validation: Ensure that the data sent by the user is valid, e.g., check if price is positive and required fields are present. âœ…

Implement Searching and Filtering: Allow users to search and filter products by name, category, or price. ğŸ”

Implement Searching and Filtering: Allow users to search and filter products by name, category, or price.
---

**Question:**
```
What should be the best practice for securing user authentication in the Marketplace service? ğŸ”’
```

**Response:**
To secure user authentication:

Use JWT (JSON Web Tokens) for user authentication. ğŸ”‘

Create an Auth Service: This service will handle login and token generation. 

Token-Based Authentication: After the user logs in, generate a JWT containing the userâ€™s identity and roles, then send this token to the user.

JWT Validation: The API Gateway will validate the JWT in each request to ensure the user is authorized to access specific resources. 

Encrypt User Data: Store passwords securely using hash algorithms like bcrypt or PBKDF2. ğŸ”

Use HTTPS: Ensure all communication between the client and server is encrypted using HTTPS to prevent data interception. ğŸŒ

Implement Searching and Filtering: Allow users to search and filter products by name, category, or price.
**Question:**
How should we consume messages from RabbitMQ in the Notification Service? Should we use @RabbitListener, or is there another method to do this?
**Response:**

In a Notification Service, consuming messages from RabbitMQ can be efficiently handled using different approaches, and yes, using @RabbitListener is a common and effective way. Here's a breakdown of how to consume messages and alternatives:

1. Using @RabbitListener (Recommended)

The @RabbitListener annotation is a straightforward and Spring-friendly way to consume messages from RabbitMQ in a Spring Boot application. It is part of the spring-boot-starter-amqp dependency, which integrates RabbitMQ with Spring.

With this approach, you can simply annotate a method with @RabbitListener and define the queue to listen to. Hereâ€™s how to do it:

@Service
public class NotificationService {

    @RabbitListener(queues = "notificationQueue")
    public void consumeMessage(String message) {
        // Process the message
        System.out.println("Received message: " + message);
    }
}

In the example above:

The @RabbitListener annotation listens to the notificationQueue.

When a message is published to that queue, the consumeMessage method gets called automatically.


Advantages of @RabbitListener:

Simplicity: It's very easy to implement and requires minimal configuration.

Declarative: It allows you to define message consumption in a declarative manner.

Concurrency support: You can configure message listener concurrency to handle multiple messages at once.

Error handling: It's easy to configure error handling and retries with annotations.


Configuration Example:

spring:
  rabbitmq:
    host: localhost
    port: 5672
    username: guest
    password: guest

2. Using MessageListenerContainer (For Customization)

If you need more control over the message consumption process, you can create a MessageListenerContainer manually. This is more advanced and allows you to customize the listener's behavior in greater detail.

Example:

@Configuration
public class RabbitConfig {

    @Bean
    public MessageListenerContainer messageListenerContainer(ConnectionFactory connectionFactory) {
        SimpleMessageListenerContainer container = new SimpleMessageListenerContainer();
        container.setConnectionFactory(connectionFactory);
        container.setQueues(new Queue("notificationQueue"));
        container.setMessageListener(new MessageListener() {
            @Override
            public void onMessage(Message message) {
                // Custom logic to handle the message
                System.out.println("Received custom message: " + new String(message.getBody()));
            }
        });
        return container;
    }
}

Advantages of MessageListenerContainer:

Full customization: More control over how the container is set up and how messages are processed.

Custom error handling and recovery: Allows you to define custom error handlers and retry strategies.


Disadvantages:

More verbose and complex than @RabbitListener.


3. Using RabbitTemplate for Polling (Less Common)

Another option is to use RabbitTemplate for manual message retrieval and processing. This approach is usually for scenarios where you want to pull messages rather than listen to them asynchronously.

Example:

@Service
public class NotificationService {

    @Autowired
    private RabbitTemplate rabbitTemplate;

    public void consumeMessage() {
        String message = (String) rabbitTemplate.receiveAndConvert("notificationQueue");
        if (message != null) {
            // Process the message
            System.out.println("Received message: " + message);
        }
    }
}

Advantages of RabbitTemplate:

Explicit control over when to consume messages.

Useful in cases where you may want to consume messages in a specific timing or rate.


Disadvantages:

Requires explicit polling and might be less efficient for high-throughput systems.


Which One Should You Use?

For simplicity and ease of use: Start with @RabbitListener. It's highly recommended for most use cases because of its simplicity, declarative nature, and integration with Spring Boot.


**Question:**
When the Notification Service receives a message from RabbitMQ, what kind of data do we need to extract from the events? For instance, how do we extract the date and time from the event?
**Response:**

Good question â€” this is one of those places where being clear early saves pain later.

When the Notification Service receives a message from RabbitMQ, it should treat the message as an event with a well-defined structure. The key idea is: the Notification Service should not guess or derive business data â€” it should extract what the producer already put into the event.


---

1. What data should we extract from an event?

At a minimum, notification events usually contain four categories of data:

1ï¸âƒ£ Event metadata (for infrastructure & debugging)

Used to understand what happened and when the event was produced.

Typical fields:

event_id â€“ unique ID (UUID)

event_type â€“ e.g. USER_REGISTERED, APPOINTMENT_REMINDER

occurred_at â€“ timestamp of the event

source â€“ service that emitted the event

version â€“ event schema version (optional but very useful)


Example:

{
  "event_id": "8f1c9e2a",
  "event_type": "APPOINTMENT_CREATED",
  "occurred_at": "2026-02-08T14:32:10Z",
  "source": "appointment-service"
}


---

2ï¸âƒ£ Recipient data (who gets notified)

The Notification Service must know who to notify.

Examples:

user_id

email

phone_number

push_token


Example:

"recipient": {
  "user_id": "12345",
  "email": "user@example.com"
}


---

3ï¸âƒ£ Notification content data (what to say)

This is business payload, used to render the notification.

Examples:

Names

Titles

Dates of appointments

Amounts

URLs


Example:

"payload": {
  "appointment_id": "a-778",
  "doctor_name": "Dr. Smith",
  "appointment_time": "2026-02-10T09:00:00+01:00"
}


---

4ï¸âƒ£ Notification rules (how & when)

Sometimes embedded, sometimes inferred:

Notification channel (email, sms, push)

Priority

Language

Scheduled vs immediate


Example:

"notification": {
  "channel": "email",
  "template": "appointment_confirmation",
  "locale": "en-US"
}


---

2. How do we extract date and time from the event?

âœ… Rule #1: Dates must be sent, not derived

The Notification Service should not calculate timestamps from context.
It should read explicit datetime fields from the event.

âœ… Rule #2: Use ISO 8601 timestamps

Dates should always be strings in ISO 8601 format.

Examples:

"2026-02-08T14:32:10Z" (UTC)

"2026-02-10T09:00:00+01:00" (with timezone)



---

Example: extracting date/time in code

Example event

{
  "event_type": "APPOINTMENT_REMINDER",
  "occurred_at": "2026-02-08T14:32:10Z",
  "payload": {
    "appointment_time": "2026-02-10T09:00:00+01:00"
  }
}

Extraction logic (conceptually)

eventOccurredAt = parseISO(event.occurred_at)
appointmentTime = parseISO(event.payload.appointment_time)

What each is used for:

occurred_at â†’ logging, retries, idempotency

appointment_time â†’ notification content (â€œYour appointment is at 9:00 AMâ€)



---

3. Which timestamp should the Notification Service trust?

Youâ€™ll often have multiple timestamps â€” thatâ€™s normal.

Field Meaning Used for

occurred_at When the event happened ordering, debugging
published_at (optional) When it hit RabbitMQ latency monitoring
business_time (e.g. appointment_time) Domain-specific time message content


ğŸ’¡ Never overload one timestamp to mean multiple things.


---

4. Best practice event contract (recommended)

{
  "event_id": "uuid",
  "event_type": "APPOINTMENT_REMINDER",
  "occurred_at": "2026-02-08T14:32:10Z",
  "recipient": {
    "user_id": "12345",
    "email": "user@example.com"
  },
  "payload": {
    "appointment_time": "2026-02-10T09:00:00+01:00",
    "doctor_name": "Dr. Smith"
  },
  "notification": {
    "channel": "email",
    "template": "appointment_reminder"
  }
}


---

5. One-sentence principle to remember

> The Notification Service extracts dates and times directly from explicit event fields, parses them using ISO 8601, and never infers or reconstructs business time on its own.



If you want, I can:

help you design an event schema for your system

show Java / Node / Python parsing examples

or explain idempotency & time handling pitfalls in RabbitMQ-based systems

**Question:**
How should we transfer messages from other services (like Booking Service) to the Notification Service? How should these messages be placed into RabbitMQ?
**Response:**

A clean way to think about it is:

Other services (e.g., Booking Service) publish â€œnotification-worthy eventsâ€

Notification Service subscribes and decides how to turn those events into user-facing notifications (email/SMS/push/in-app)


That keeps â€œbusiness happenedâ€ separate from â€œhow we notifyâ€.

1) What to transfer: publish events, not â€œsend notificationâ€ commands

From Booking Service, publish domain events like:

booking.created

booking.confirmed

booking.cancelled

payment.succeeded

payment.failed


Avoid publishing â€œSendEmailToUserâ€ commands from Booking Service unless you truly want Booking to control messaging logic. Usually Notification Service should own templates, channels, throttling, etc.

2) Reliability: use the Transactional Outbox pattern

Biggest pitfall: you update your DB, then fail to publish to RabbitMQ (or publish twice).

Best practice in microservices with RabbitMQ:

1. Booking Service writes business change and an outbox row in the same DB transaction.


2. A background publisher (same service) reads the outbox and publishes to RabbitMQ.


3. Only after broker confirms publish (publisher confirms), mark outbox row as sent.



This gives you at-least-once delivery safely.

(Alternative if you already have CDC like Debezium: publish outbox via CDC, but same idea.)

3) How to â€œplace messages into RabbitMQâ€: Exchanges, routing keys, queues

Use this topology:

Exchange

Create a topic exchange for business events, e.g.

events (type: topic, durable)


Routing keys

Use consistent routing keys:

booking.confirmed

booking.cancelled

payment.succeeded


Queues (owned by consumers)

Notification Service should have its own queue (durable). Example:

notification.booking bound to events with binding keys:

booking.*

payment.* (if it also needs payment events)



If you have multiple consumers (Analytics, CRM, etc.), each gets its own queue bound to the same exchange. Thatâ€™s the main win of pub/sub with RabbitMQ.

4) Message format: use a stable envelope + versioning

Publish JSON with an envelope (similar to CloudEvents). Example:

{
  "specversion": "1.0",
  "type": "booking.confirmed",
  "source": "booking-service",
  "id": "0f3b3c1e-4c2c-4e7a-9a33-2d6d0c7c3a1b",
  "time": "2026-02-08T10:15:30Z",
  "subject": "booking/12345",
  "datacontenttype": "application/json",
  "data": {
    "bookingId": "12345",
    "userId": "u-777",
    "hotelId": "h-42",
    "checkIn": "2026-03-01",
    "checkOut": "2026-03-05"
  }
}

Key points:

type is your routing/event name

id is unique (used for deduplication)

data is the payload

Add schemaVersion (or embed in type like booking.confirmed.v2) when you evolve fields


5) AMQP properties + durability settings

When publishing:

delivery_mode = 2 (persistent)

content_type = application/json

message_id = <event id>

correlation_id = <request/trace id> (optional but very useful)

headers:

x-event-type: booking.confirmed

x-schema-version: 1



On RabbitMQ:

exchange: durable

queue: durable

prefer quorum queues for higher resilience (vs classic), if your ops team supports it


6) Consumer behavior in Notification Service: idempotency + acking

Because outbox gives at-least-once, Notification Service must be able to handle duplicates.

Recommended:

Store processed message_id/event id in a small table (or Redis set with TTL)

Only ACK after:

youâ€™ve persisted whatever notification job you created, or

youâ€™ve successfully handed off to your internal notification pipeline



If processing fails:

NACK/requeue for transient failures

For poison messages, route to a dead-letter queue (DLQ)


7) Retries, DLQs, and delayed retries

Use:

Main queue: notification.booking

DLQ: notification.booking.dlq

Optional retry queue(s) with TTL (or a delayed exchange plugin if you use it)


Pattern without plugins:

notification.booking.retry.30s (TTL 30s â†’ dead-letter back to main)

notification.booking.retry.5m etc.


This avoids hammering downstream SMS/email providers.

8) What NOT to do

**Question:**
Should we use JavaMail for sending emails, or is there a better tool for email delivery in Spring Boot that works well with this type of system?
**Response:**

JavaMail (via Springâ€™s JavaMailSender) works and is fine for basic use, but for anything beyond simple notifications it isnâ€™t usually the best choice for production email delivery.

Hereâ€™s a clear comparison and recommendations based on typical needs:


---

ğŸ§  1. JavaMailSender (Spring Boot) â€” Basic and Localâ€‘First

Use if:

You are sending only a few emails.

You donâ€™t need deliverability analytics (opens, bounces, spam rates).

Youâ€™re okay handling retries, throttling, and error details yourself.


Pros âœ” Simple and builtâ€‘in (comes with Spring Boot)
âœ” No thirdâ€‘party dependency
âœ” Great for development/test environments

Cons âŒ No builtâ€‘in deliverability features
âŒ Sending large volume = maintenance headaches
âŒ SMTP servers can block or rateâ€‘limit your app
âŒ Hard to track metrics (opens, clicks, bounces)

Example

@Autowired
private JavaMailSender mailSender;

public void sendEmail(String to, String subject, String text) {
    SimpleMailMessage msg = new SimpleMailMessage();
    msg.setTo(to);
    msg.setSubject(subject);
    msg.setText(text);
    mailSender.send(msg);
}


---

ğŸ“§ 2. External Email Delivery Services (Recommended for Production)

These services manage deliverability, reputation, scaling, retries, and insights â€” which JavaMail doesnâ€™t.

Best Options

Provider Easy Integration Deliverability Analytics Free Tier

SendGrid â­â­â­â­ â­â­â­â­ â­â­â­â­ Yes
Mailgun â­â­â­â­ â­â­â­â­ â­â­â­â­ Yes
Amazon SES â­â­â­ â­â­â­â­â­ â­â­â­ Yes
Postmark â­â­â­ â­â­â­â­ â­â­ Limited
SparkPost â­â­â­â­ â­â­â­â­ â­â­â­ Yes


ğŸ‘‰ Amazon SES is very costâ€‘effective at scale
ğŸ‘‰ SendGrid/Mailgun are beginnerâ€‘friendly with great APIs


---

ğŸ›  How It Works in Spring Boot

Instead of configuring SMTP directly with JavaMail, you send email via the providerâ€™s API:

Example: SendGrid Integration

1. Add dependency

<dependency>
    <groupId>com.sendgrid</groupId>
    <artifactId>sendgrid-java</artifactId>
    <version>4.9.3</version>
</dependency>

2. Use it in your service

@Service
public class EmailService {
    private final SendGrid sendGrid;

    public EmailService(@Value("${sendgrid.api.key}") String apiKey) {
        this.sendGrid = new SendGrid(apiKey);
    }

    public void send(String to, String subject, String body) throws IOException {
        Email from = new Email("no-reply@yourapp.com");
        Email toEmail = new Email(to);
        Content content = new Content("text/html", body);
        Mail mail = new Mail(from, subject, toEmail, content);

        Request request = new Request();
        request.setMethod(Method.POST);
        request.setEndpoint("mail/send");
        request.setBody(mail.build());
        sendGrid.api(request);
    }
}


---

ğŸš€ Benefits of Using Email Delivery Services

âœ… Higher deliverability and better spam filtering
âœ… Reputation & IP management handled for you
âœ… Tracking: opens, clicks, bounces, complaints
âœ… Templates & personalization support
âœ… Scales easily with your appâ€™s growth


---

ğŸ“Œ When to Still Use JavaMailSender

Internal tools where deliverability isnâ€™t critical

Local testing / staging environments

Prototyping features quickly



---

ğŸ§© Recommended Setup Pattern

Environment Recommended Tool

Development / Tests JavaMailSender (dev SMTP like MailHog)
Production (low volume) Mailgun or SendGrid
Production (high volume) Amazon SES (cheapest + scalable)



---

ğŸ“¥ Bonus Tips

ğŸ”¥ Always verify SPF/DKIM/DMARC DNS records for your sending domain
ğŸ”¥ Use email templates and personalization tokens
ğŸ”¥ Implement retry & error logging
ğŸ”¥ Monitor bounces and unsubscribes


---

ğŸ Summary

Tool Best For Overall

JavaMailSender Simple, small volume ğŸ‘ Good for dev
SendGrid/Mailgun Most web apps â­â­â­â­ Better choice
Amazon SES Costâ€‘effective large volume â­â­â­â­â­ Best at scale



---

If you want, I can tailor a recommendation based on your expected email volume and use case â€” just tell me what you plan to send and how often!

**Question:**
Do we need to configure any specific security settings for the SMTP server to send emails? For example, should we use TLS or other protocols?
**Response:**

Yepâ€”there are some important security settings you should configure for an SMTP server. Think of this as the â€œdonâ€™t send passwords in plain text over the internetâ€ starter pack ğŸ˜„

Hereâ€™s whatâ€™s considered best practice today:


---

1. Use TLS encryption (this is a must)

You should absolutely use TLS to encrypt emails in transit between your app and the SMTP server.

Youâ€™ll usually see one of these two setups:

âœ… STARTTLS (most common)

Connects on port 587

Starts unencrypted, then upgrades to TLS

Recommended by most providers (Gmail, Microsoft, SES, etc.)


Best practice:

Enable STARTTLS

Require it (donâ€™t allow fallback to plain text)


âœ… Implicit TLS (SMTPS)

Connects on port 465

Encrypted from the start

Still widely supported


Either is fineâ€”587 + STARTTLS is generally preferred unless your provider says otherwise.


---

2. Never allow plain-text SMTP

Avoid port 25 for sending mail from applications:

Itâ€™s often blocked by ISPs

No encryption by default

Commonly abused for spam


Port 25 is mostly for server-to-server mail transfer, not apps.


---

3. Enable SMTP authentication

Your SMTP server should require authentication:

Username + password

Or API key (preferred when supported)


And make sure:

Auth credentials are only sent over TLS

You donâ€™t allow anonymous sending



---

4. Use strong authentication methods

If configurable, prefer:

AUTH LOGIN / PLAIN over TLS (acceptable)

OAuth 2.0 (best, if supportedâ€”e.g., Gmail, Microsoft)


Avoid:

Legacy auth without encryption

Shared passwords across services



---

5. Verify TLS certificates

Your application should:

Validate the SMTP serverâ€™s TLS certificate

Reject invalid or self-signed certs (unless you really know what youâ€™re doing)


This prevents man-in-the-middle attacks.


---

6. Lock down the SMTP server (if you manage it)

If this is your own SMTP server:

Restrict sending to known IPs or networks

Rate-limit outgoing mail

Disable open relay (critical!)

Log and monitor authentication attempts



---

7. Bonus: email trust & deliverability

Not strictly SMTP security, but very important:

Configure:

SPF

DKIM

DMARC


These help prevent spoofing and keep your emails out of spam folders.


---

TL;DR (recommended setup)

âœ… Port 587

âœ… STARTTLS required

âœ… SMTP authentication enabled

âœ… Certificate validation on

ğŸš« No plain-text SMTP

ğŸš« No anonymous sending


If you want, tell me:

What SMTP provider youâ€™re using (Gmail, Microsoft, Postfix, SES, etc.)

Whether this is for an app, website, or internal system

I can give you exact config examples.


The User-Service
---

**Question:**
The current design has no caching on profile reads. If frequent profile lookups are expected (e.g., for dashboard), where and how would you add caching (e.g., Spring Cache + Redis)? Discuss eviction strategy, cache key design, and invalidation triggers.

**Answer:**

In the current implementation of the User Service, profile reads (such as fetching a user's full name, student number, or other profile details for dashboards or other views) always hit the database directly. This is acceptable for low-traffic scenarios but becomes inefficient when profile lookups are frequent, as is expected in a university platform (e.g., student dashboard on login, professor viewing class lists, administrative panels loading user details, or mobile app refreshing profile data).
To address this performance concern while respecting the non-functional requirement of low latency (<400 ms for 95% of API calls), caching should be introduced specifically for read operations. The recommended approach is to use the Spring Cache Abstraction combined with Redis as the backing cache store. This is a standard, mature, and production-ready pattern in Spring Boot microservices and aligns perfectly with the project's existing technology stack (Spring Boot, Redis already present in docker-compose for other services).
Where to Place Caching
Caching must be applied at the service layer, not in the controller or repository. Placing it in the controller bypasses business logic and validation. Placing it directly in the repository (e.g., via second-level cache) is more complex, less flexible, and harder to control invalidation.
The ideal location is a read method in the UserProfileService, such as a future getProfileByEmail or getMyProfile method. This keeps caching close to the business logic while still benefiting from repository-level data access.
How to Implement Caching
First, add the necessary dependencies to the service's build file (Gradle in this case). Include the Spring Cache starter and the Redis starter. Enable caching globally by adding the appropriate annotation to the main application class.
Next, configure Redis as the cache provider. This is typically done through application properties or a dedicated configuration class, specifying the Redis host (the service name in docker-compose), port, and optional connection settings. Define a default cache configuration, such as serialization strategy (JSON for objects) and null-value handling (to avoid caching null results).
Then, apply caching annotations directly to service methods that perform expensive reads. Use the @Cacheable annotation on the read method, specifying a cache name (a logical group, e.g., "userProfiles") and a key expression (usually based on the method parameter, such as the email).
Cache Key Design
The cache key must be unique per profile and fast to compute. The most natural and efficient key is the email address (since it is already the uniqueness constraint in the entity). This results in simple, human-readable keys like "userProfiles::student@example.com".
If multiple parameters are needed in the future (e.g., combining email and tenantId for stricter isolation), use a composite key expression. For even more complex cases, a custom key generator can be implemented, but it is rarely needed here.
Eviction Strategy
The cache should not live forever. A reasonable time-to-live (TTL) is 30 minutes to 1 hour for profile data, as profiles do not change very frequently (e.g., name or student number updates are rare compared to reads). This TTL can be set globally for all caches or overridden per cache name.
For more aggressive eviction, a least-recently-used (LRU) policy can be configured in Redis, but TTL is usually sufficient and simpler.
Invalidation Triggers
The most critical part of caching is keeping the cache consistent with the database. Whenever a profile is created, updated, or deleted, the corresponding cache entry must be invalidated immediately.
This is achieved using the @CacheEvict annotation on create, update, and delete methods in the service layer. The key expression must match the one used in @Cacheable (e.g., based on email). For create operations, evict using the email from the request DTO. For update operations, evict using the email from the updated entity or request.
In rare cases where bulk operations are added, a global eviction (allEntries = true) can be used, but it should be avoided as it reduces cache hit rate unnecessarily.
Monitoring and Testing
To ensure the cache behaves correctly, expose cache metrics via Spring Boot Actuator (already included in most starters). Add endpoints for health and metrics, and monitor cache hit rate, miss rate, and eviction count in production.
For testing, write integration tests that verify cache hits (same input returns from cache without database query) and evictions (update invalidates cache, next read hits database again).
Summary of Benefits and Considerations
Adding caching at the service layer with Spring Cache + Redis significantly reduces database load for read-heavy operations, improves response times (often <50 ms for cache hits), and enhances scalability without changing the core business logic. The design is simple to implement, easy to monitor, and safe when invalidation is correctly tied to write operations.
This enhancement directly supports the project's performance non-functional requirement and prepares the service for higher load scenarios (e.g., enrollment periods or campus-wide dashboard usage). It is a low-risk, high-reward improvement that can be added incrementally after basic functionality is stable.

**Question:**
Why is the User entity kept in user-service despite most authentication logic being in auth-service? What technical criteria would make you decide to remove it entirely from user-service?

**Answer:**
In the current architecture of the university management platform, the User entity is retained in the user-service even though the majority of authentication-related logic (credential storage, password hashing, login, token issuance, and basic identity validation) resides in the auth-service. This design decision reflects a pragmatic compromise between domain purity, operational simplicity, and short-term implementation constraints, but it is not the ideal long-term state. Below is a detailed technical analysis of why the entity is kept and the specific criteria that would justify removing it entirely.
Reasons the User Entity Is Kept in User-Service (Current Rationale)

Spring Security Integration Dependency
The User class implements Spring Security's UserDetails interface, which requires methods such as getAuthorities(), getUsername(), getPassword(), and the account status flags (isEnabled(), etc.). These methods are used directly by Spring Security's authentication providers (e.g., DaoAuthenticationProvider or custom UserDetailsService) to load and validate users during login and token generation.
If the User entity were removed from user-service, either:
user-service would need to depend on a shared UserDetails implementation from auth-service (introducing tight coupling via a shared library), or
user-service would have to duplicate or proxy the UserDetails logic, violating DRY and increasing maintenance cost.
Keeping the entity avoids this duplication and allows user-service to participate in authentication flows (e.g., role-based authorization checks after login) without external dependencies.

Role and Tenant Information Needed Locally
The User entity includes the role field (enum-mapped) and tenantId. These fields are used in user-service for:
Role-based filtering of profile data or actions (e.g., students cannot modify professor profiles).
Multi-tenancy enforcement (e.g., query scoping by tenantId to prevent cross-faculty data leakage).
Fetching these fields from auth-service on every request would introduce latency and a synchronous dependency, violating the loose-coupling requirement. Storing them locally in user-service allows fast, local decisions.

Short-Term Development and Testing Simplicity
During early prototyping and local development, having the User entity in user-service simplifies testing (e.g., creating test users with roles directly in user-service tests, running integration tests without spinning up auth-service). It also reduces the need for complex inter-service mocking or contract testing in the initial phases.
Avoiding Premature Shared Library
Creating a shared library (e.g., common-domain module with User and Role) would introduce a new dependency between services. While this is the clean long-term solution, it adds build complexity, version management, and deployment coordination â€” overhead that is unnecessary for a student project prototype.

Technical Criteria That Would Justify Removing User Entity from User-Service Entirely
The User entity should be removed from user-service when one or more of the following technical conditions are met:

Authentication Is Fully Isolated and Never Reused in User-Service
If user-service no longer needs to perform any authentication or authorization logic that requires direct access to credentials, roles, or account status, the entity becomes redundant.
Criterion: All role-based decisions in user-service are made solely based on JWT claims (passed via Gateway) rather than local User lookup.
Roles and TenantId Are Reliably Available via JWT Claims
If the JWT issued by auth-service consistently includes the full set of required claims (role, tenantId, etc.) and the token is validated and parsed at the Gateway level (or in a shared filter), user-service can extract these values from the security context without needing a local User entity.
Criterion: The Gateway enforces role-based routing or header enrichment, making local role storage unnecessary.
Introduction of a Shared Domain Library
When the project matures and a common-domain module is created (containing Role enum, UserId value object, etc.), user-service can depend on this library instead of owning the User entity.
Criterion: Shared types (Role, TenantId) are extracted to a separate Maven/Gradle module published to a repository or included as a multi-module project dependency.
Migration to Event-Driven Synchronization
Once registration becomes event-driven (auth-service publishes UserRegistered or UserUpdated events containing role and tenantId), user-service can store only the profile-specific data and fetch role/tenant information on-demand from events or a read-only cache.
Criterion: Event schema includes all necessary identity attributes, and user-service consumes events to keep a local projection (CQRS-style).
Performance and Consistency Requirements Outweigh Local Storage
If profiling shows that local role/tenant lookups cause consistency problems (e.g., role changes in auth-service are not reflected in user-service), or if the overhead of maintaining duplicated data becomes significant, removal becomes preferable.
Criterion: Monitoring shows high write contention on duplicated fields or unacceptable staleness.

Recommended Long-Term Direction
The ideal end-state is to remove the User entity from user-service entirely once the following are in place:

JWT includes role and tenantId claims (or they are enriched by Gateway).
Role-based authorization is enforced at Gateway or via a shared security filter.
Profile creation is triggered by events (Saga or choreography).
Shared domain types (Role enum, value objects) are moved to a common library.

Until then, retaining User in user-service is a pragmatic compromise that enables local development, simplifies testing, and avoids premature abstraction. It is not a design flaw â€” it is an evolutionary step.
In summary, the entity is kept today to support immediate authentication integration and local decision-making. It should be removed when identity data is fully externalized via tokens/events and shared types, shifting user-service toward a pure profile-management responsibility. This evolution would further improve loose coupling, reduce duplication, and align more closely with microservices best practices.

---
